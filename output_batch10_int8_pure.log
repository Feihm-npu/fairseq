2024-07-12 03:45:10 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:18848
2024-07-12 03:45:10 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:18848
2024-07-12 03:45:10 | INFO | fairseq.distributed.utils | initialized host t006-001.hpcfund as rank 3
2024-07-12 03:45:10 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:18848
2024-07-12 03:45:10 | INFO | fairseq.distributed.utils | initialized host t006-001.hpcfund as rank 1
2024-07-12 03:45:10 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:18848
2024-07-12 03:45:10 | INFO | fairseq.distributed.utils | initialized host t006-001.hpcfund as rank 2
2024-07-12 03:45:10 | INFO | fairseq.distributed.utils | initialized host t006-001.hpcfund as rank 0
/work1/amd/hongmfei/moespace/SPEED-main/fairseq/fairseq/distributed/utils.py:279: UserWarning: expandable_segments not supported on this platform (Triggered internally at ../c10/hip/HIPAllocatorConfig.h:29.)
  dist.all_reduce(torch.zeros(1).cuda())
/work1/amd/hongmfei/moespace/SPEED-main/fairseq/fairseq/distributed/utils.py:279: UserWarning: expandable_segments not supported on this platform (Triggered internally at ../c10/hip/HIPAllocatorConfig.h:29.)
  dist.all_reduce(torch.zeros(1).cuda())
/work1/amd/hongmfei/moespace/SPEED-main/fairseq/fairseq/distributed/utils.py:279: UserWarning: expandable_segments not supported on this platform (Triggered internally at ../c10/hip/HIPAllocatorConfig.h:29.)
  dist.all_reduce(torch.zeros(1).cuda())
/work1/amd/hongmfei/moespace/SPEED-main/fairseq/fairseq/distributed/utils.py:279: UserWarning: expandable_segments not supported on this platform (Triggered internally at ../c10/hip/HIPAllocatorConfig.h:29.)
  dist.all_reduce(torch.zeros(1).cuda())
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | ---------------------------
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | cfg:
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | _name	None
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | common	{'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 100, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'log_nvidia_smi': False}
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | common_eval	{'_name': None, 'path': '/work1/amd/hongmfei/models/en_moe_lm_15b/model.pt', 'post_process': None, 'quiet': False, 'model_overrides': "{'world_size': 4, 'moe_eval_capacity_token_fraction': 0.05}", 'results_path': None, 'is_moe': True}
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | distributed_training	{'_name': None, 'distributed_world_size': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18848', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'fully_sharded', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'distributed_num_procs': 4}
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | dataset	{'_name': None, 'num_workers': 1, 'num_workers_valid': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 10, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 10, 'max_valid_steps': 10, 'curriculum': 0, 'gen_subset': 'valid', 'num_shards': 1, 'shard_id': 0}
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | optimization	{'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False}
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | checkpoint	{'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_best_checkpoints': False, 'no_save_optimizer_state': False, 'no_save_optimizer_state_on_training_finished': False, 'symlink_best_and_last_checkpoints': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 's3_upload_path': None, 'model_parallel_size': 1}
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | bmuf	{'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | generation	{'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | eval_lm	{'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 2048, 'stats_path': None, 'max_valid_steps': 10}
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | interactive	{'_name': None, 'buffer_size': 0, 'input': '-'}
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | model	None
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | task	{'_name': 'language_modeling', 'data': '/work1/amd/hongmfei/raw_data/data-bin/wiki/', 'sample_break_mode': 'none', 'tokens_per_sample': 2048, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_source_positions': None, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 100, 'batch_size': 10, 'batch_size_valid': 10, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | criterion	{'_name': 'cross_entropy', 'sentence_avg': True}
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | optimizer	None
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | lr_scheduler	{'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | scoring	{'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | bpe	{'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | tokenizer	None
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | simul_type	None
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | ---------------------------
2024-07-12 03:45:11 | INFO | fairseq_cli.eval_lm | loading model(s) from /work1/amd/hongmfei/models/en_moe_lm_15b/model.pt
2024-07-12 03:45:11 | INFO | fairseq.tasks.language_modeling | dictionary: 51200 types
2024-07-12 03:45:11 | INFO | fairseq.checkpoint_utils | load_model_ensemble_and_task is_moe=True
2024-07-12 03:45:11 | INFO | fairseq.moe_checkpoint_utils | Found total 64 expert files and current distributed world size: 4, Stitching experts to able to load on current world size.
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=0
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=1
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=2
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=3
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=4
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=5
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=6
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=7
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=8
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=9
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=10
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=11
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=12
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=13
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=14
2024-07-12 03:45:13 | INFO | fairseq.moe_checkpoint_utils | found 8 local experts in expert_group_id=15
2024-07-12 03:45:16 | INFO | fairseq.modules.moe.moe_layer | Rank: 0
2024-07-12 03:45:19 | INFO | fairseq.modules.moe.moe_layer | Rank: 0
2024-07-12 03:45:21 | INFO | fairseq.modules.moe.moe_layer | Rank: 0
2024-07-12 03:45:24 | INFO | fairseq.modules.moe.moe_layer | Rank: 0
2024-07-12 03:45:27 | INFO | fairseq.modules.moe.moe_layer | Rank: 0
2024-07-12 03:45:29 | INFO | fairseq.modules.moe.moe_layer | Rank: 0
2024-07-12 03:45:31 | INFO | fairseq_cli.eval_lm | num. model params: 3,725,230,080
2024-07-12 03:45:38 | INFO | fairseq.data.data_utils | loaded 1,482,831,152 examples from: /work1/amd/hongmfei/raw_data/data-bin/wiki/valid
2024-07-12 03:45:46 | INFO | fairseq_cli.eval_lm | /work1/amd/hongmfei/raw_data/data-bin/wiki/ valid 17,792,645 examples
2024-07-12 03:45:51 | INFO | fairseq_cli.eval_lm | load time: 40.51 seconds
/work1/amd/hongmfei/moespace/.moefair/lib64/python3.9/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/work1/amd/hongmfei/moespace/.moefair/lib64/python3.9/site-packages/torch/nn/functional.py:5504: UserWarning: 1Torch was not compiled with memory efficient attention. (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:505.)
  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
Warning: The --amdgpu-target option has been deprecated and will be removed in the future.  Use --offload-arch instead.
/work1/amd/hongmfei/moespace/.moefair/lib64/python3.9/site-packages/torch/nn/functional.py:5504: UserWarning: 1Torch was not compiled with memory efficient attention. (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:505.)
  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
Warning: The --amdgpu-target option has been deprecated and will be removed in the future.  Use --offload-arch instead.
Warning: The --amdgpu-target option has been deprecated and will be removed in the future.  Use --offload-arch instead.
Warning: The --amdgpu-target option has been deprecated and will be removed in the future.  Use --offload-arch instead.
/work1/amd/hongmfei/moespace/.moefair/lib64/python3.9/site-packages/torch/nn/functional.py:5504: UserWarning: 1Torch was not compiled with memory efficient attention. (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:505.)
  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
Warning: The --amdgpu-target option has been deprecated and will be removed in the future.  Use --offload-arch instead.
/work1/amd/hongmfei/moespace/.moefair/lib64/python3.9/site-packages/torch/nn/functional.py:5504: UserWarning: 1Torch was not compiled with memory efficient attention. (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:505.)
  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
Warning: The --amdgpu-target option has been deprecated and will be removed in the future.  Use --offload-arch instead.
Warning: The --amdgpu-target option has been deprecated and will be removed in the future.  Use --offload-arch instead.
Warning: The --amdgpu-target option has been deprecated and will be removed in the future.  Use --offload-arch instead.
Warning: The --amdgpu-target option has been deprecated and will be removed in the future.  Use --offload-arch instead.
Warning: The --amdgpu-target option has been deprecated and will be removed in the future.  Use --offload-arch instead.
Warning: The --amdgpu-target option has been deprecated and will be removed in the future.  Use --offload-arch instead.
Warning: The --amdgpu-target option has been deprecated and will be removed in the future.  Use --offload-arch instead.
now round: 1
New cuda time without quantization: 4965.59326171875
torch.Size([524288, 768]) torch.qint8
New cuda time: 4976.69091796875
New cuda time without quantization: 7.998746871948242
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 18.13797950744629
now round: 1
New cuda time without quantization: 198.75698852539062
torch.Size([524288, 768]) torch.qint8
New cuda time: 208.82742309570312
New cuda time without quantization: 8.01298713684082
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 18.139739990234375
now round: 1
New cuda time without quantization: 6.785462856292725
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.014616012573242
New cuda time without quantization: 7.179704189300537
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.240375518798828
now round: 1
New cuda time without quantization: 11.224516868591309
torch.Size([524288, 768]) torch.qint8
New cuda time: 21.23447036743164
New cuda time without quantization: 7.526744842529297
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.569337844848633
now round: 1
New cuda time without quantization: 11.337957382202148
torch.Size([524288, 768]) torch.qint8
New cuda time: 21.385032653808594
New cuda time without quantization: 18.73766326904297
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 28.804895401000977
now round: 1
New cuda time without quantization: 11.116836547851562
torch.Size([524288, 768]) torch.qint8
New cuda time: 21.13302993774414
New cuda time without quantization: 6.608022212982178
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.760215759277344
now round: 2
New cuda time without quantization: 8.690749168395996
torch.Size([524288, 768]) torch.qint8
New cuda time: 18.727901458740234
New cuda time without quantization: 7.553625106811523
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.65589714050293
now round: 2
New cuda time without quantization: 7.067223072052002
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.114456176757812
New cuda time without quantization: 17.86389923095703
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.916732788085938
now round: 2
New cuda time without quantization: 13.598925590515137
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.632558822631836
New cuda time without quantization: 7.688025951385498
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.72549819946289
now round: 2
New cuda time without quantization: 7.272503852844238
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.281335830688477
New cuda time without quantization: 6.678903102874756
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.756855010986328
now round: 2
New cuda time without quantization: 7.111382961273193
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.139575958251953
New cuda time without quantization: 7.532185077667236
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.58949851989746
now round: 2
New cuda time without quantization: 7.182743072509766
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.20293617248535
New cuda time without quantization: 18.397661209106445
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 28.48393440246582
now round: 3
New cuda time without quantization: 14.251248359680176
torch.Size([524288, 768]) torch.qint8
New cuda time: 24.274959564208984
New cuda time without quantization: 7.558104991912842
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 24.07784080505371
now round: 3
New cuda time without quantization: 6.758102893829346
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.77925682067871
New cuda time without quantization: 7.524664878845215
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.630138397216797
now round: 3
New cuda time without quantization: 7.034102916717529
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.069976806640625
New cuda time without quantization: 7.622106075286865
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.649497985839844
now round: 3
New cuda time without quantization: 7.167383193969727
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.19477653503418
New cuda time without quantization: 19.735746383666992
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 29.802019119262695
now round: 3
New cuda time without quantization: 6.933463096618652
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.926776885986328
New cuda time without quantization: 6.74802303314209
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.952856063842773
now round: 3
New cuda time without quantization: 6.931702136993408
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.93573570251465
New cuda time without quantization: 7.513625144958496
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.652538299560547
now round: 4
New cuda time without quantization: 7.229944229125977
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.222936630249023
New cuda time without quantization: 7.553464889526367
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.59685707092285
now round: 4
New cuda time without quantization: 18.05190086364746
torch.Size([524288, 768]) torch.qint8
New cuda time: 28.05177116394043
New cuda time without quantization: 10.828676223754883
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 20.92422866821289
now round: 4
New cuda time without quantization: 7.08898401260376
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.09685516357422
New cuda time without quantization: 6.855062961578369
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.94229507446289
now round: 4
New cuda time without quantization: 7.013623237609863
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.037975311279297
New cuda time without quantization: 7.576984882354736
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.636537551879883
now round: 4
New cuda time without quantization: 7.112823009490967
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.11893653869629
New cuda time without quantization: 17.66390037536621
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.735931396484375
now round: 4
New cuda time without quantization: 13.573165893554688
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.580238342285156
New cuda time without quantization: 7.650105953216553
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.706939697265625
now round: 5
New cuda time without quantization: 6.841302871704102
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.893495559692383
New cuda time without quantization: 7.538905143737793
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.581018447875977
now round: 5
New cuda time without quantization: 6.99202299118042
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.01669692993164
New cuda time without quantization: 7.6045050621032715
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.647258758544922
now round: 5
New cuda time without quantization: 7.1405029296875
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.13813591003418
New cuda time without quantization: 17.464860916137695
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.52473258972168
now round: 5
New cuda time without quantization: 7.092023849487305
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.09685516357422
New cuda time without quantization: 6.69202184677124
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.78277587890625
now round: 5
New cuda time without quantization: 7.095542907714844
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.112375259399414
New cuda time without quantization: 7.463864803314209
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.546138763427734
now round: 5
New cuda time without quantization: 7.266424179077148
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.267736434936523
New cuda time without quantization: 7.730425834655762
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.77189826965332
now round: 6
New cuda time without quantization: 18.21381950378418
now round: 1
New cuda time without quantization: 475.7621765136719
torch.Size([524288, 768]) torch.qint8
New cuda time: 486.64007568359375
New cuda time without quantization: 7.591178894042969
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.751628875732422
now round: 1
New cuda time without quantization: 6.779820919036865
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.843952178955078
New cuda time without quantization: 6.7129411697387695
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.865711212158203
now round: 1
New cuda time without quantization: 12.919482231140137
torch.Size([524288, 768]) torch.qint8
New cuda time: 22.972095489501953
New cuda time without quantization: 6.996300220489502
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.054031372070312
now round: 1
New cuda time without quantization: 10.912927627563477
torch.Size([524288, 768]) torch.qint8
New cuda time: 20.927139282226562
New cuda time without quantization: 6.814380168914795
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.983631134033203
now round: 1
New cuda time without quantization: 10.92460823059082
torch.Size([524288, 768]) torch.qint8
New cuda time: 20.958179473876953
New cuda time without quantization: 18.22410774230957
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 28.29863929748535
now round: 1
New cuda time without quantization: 10.634369850158691
torch.Size([524288, 768]) torch.qint8
New cuda time: 20.68474006652832
New cuda time without quantization: 11.579647064208984
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.843616485595703
now round: 2
New cuda time without quantization: 7.172938823699951
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.246030807495117
New cuda time without quantization: 6.897420883178711
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.053071975708008
now round: 2
New cuda time without quantization: 6.848138809204102
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.86123275756836
New cuda time without quantization: 17.095151901245117
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.15144157409668
now round: 2
New cuda time without quantization: 13.404122352600098
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.483291625976562
New cuda time without quantization: 7.156459808349609
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.222190856933594
now round: 2
New cuda time without quantization: 6.8129401206970215
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.84939193725586
New cuda time without quantization: 11.141728401184082
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.245859146118164
now round: 2
New cuda time without quantization: 6.775821208953857
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.81211280822754
New cuda time without quantization: 6.966219902038574
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.013872146606445
now round: 2
New cuda time without quantization: 6.817739963531494
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.842031478881836
New cuda time without quantization: 17.779630661010742
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.876079559326172
now round: 3
New cuda time without quantization: 13.875161170959473
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.893692016601562
New cuda time without quantization: 6.891500949859619
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.055152893066406
now round: 3
New cuda time without quantization: 12.691802978515625
torch.Size([524288, 768]) torch.qint8
New cuda time: 22.742015838623047
New cuda time without quantization: 6.800780773162842
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.868751525878906
now round: 3
New cuda time without quantization: 6.836938858032227
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.841232299804688
New cuda time without quantization: 6.9268598556518555
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.010351181030273
now round: 3
New cuda time without quantization: 6.8550190925598145
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.884111404418945
New cuda time without quantization: 18.87578582763672
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 28.946636199951172
now round: 3
New cuda time without quantization: 6.835659980773926
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.850831985473633
New cuda time without quantization: 12.288764953613281
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.37241554260254
now round: 3
New cuda time without quantization: 6.79486083984375
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.825712203979492
New cuda time without quantization: 6.922220230102539
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.000431060791016
now round: 4
New cuda time without quantization: 6.964620113372803
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.99291229248047
New cuda time without quantization: 6.930220127105713
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.000112533569336
now round: 4
New cuda time without quantization: 15.387795448303223
torch.Size([524288, 768]) torch.qint8
New cuda time: 25.641048431396484
New cuda time without quantization: 9.860132217407227
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 19.950342178344727
now round: 4
New cuda time without quantization: 6.8110198974609375
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.833391189575195
New cuda time without quantization: 12.858202934265137
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.95017433166504
now round: 4
New cuda time without quantization: 6.609100818634033
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.646671295166016
New cuda time without quantization: 6.893099784851074
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.948911666870117
now round: 4
New cuda time without quantization: 6.959019184112549
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.978031158447266
New cuda time without quantization: 16.967472076416016
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.129043579101562
now round: 4
New cuda time without quantization: 13.183483123779297
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.19033432006836
New cuda time without quantization: 6.880939960479736
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.942832946777344
now round: 5
New cuda time without quantization: 12.009406089782715
torch.Size([524288, 768]) torch.qint8
New cuda time: 22.02025604248047
New cuda time without quantization: 6.8004608154296875
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.860591888427734
now round: 5
New cuda time without quantization: 6.839018821716309
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.855791091918945
New cuda time without quantization: 6.893420219421387
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.955631256103516
now round: 5
New cuda time without quantization: 6.856618881225586
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.90235137939453
New cuda time without quantization: 16.815311431884766
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 26.90472412109375
now round: 5
New cuda time without quantization: 6.814700126647949
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.871152877807617
New cuda time without quantization: 11.125247955322266
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.206180572509766
now round: 5
New cuda time without quantization: 6.847661018371582
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.837392807006836
New cuda time without quantization: 6.886539936065674
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.964111328125
now round: 5
New cuda time without quantization: 6.97261905670166
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.020431518554688
New cuda time without quantization: 6.907179832458496
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.96219253540039
now round: 6
New cuda time without quantization: now round: 1
New cuda time without quantization: 920.4304809570312
torch.Size([524288, 768]) torch.qint8
New cuda time: 931.406494140625
New cuda time without quantization: 8.842096328735352
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 19.061153411865234
now round: 1
New cuda time without quantization: 7.846573829650879
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.96227264404297
New cuda time without quantization: 7.294733047485352
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.509471893310547
now round: 1
New cuda time without quantization: 12.955862998962402
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.01572036743164
New cuda time without quantization: 6.9078521728515625
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.972349166870117
now round: 1
New cuda time without quantization: 10.981779098510742
torch.Size([524288, 768]) torch.qint8
New cuda time: 21.001155853271484
New cuda time without quantization: 6.936172962188721
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.113950729370117
now round: 1
New cuda time without quantization: 10.880338668823242
torch.Size([524288, 768]) torch.qint8
New cuda time: 20.940996170043945
New cuda time without quantization: 6.885451793670654
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.990589141845703
now round: 1
New cuda time without quantization: 10.920019149780273
torch.Size([524288, 768]) torch.qint8
New cuda time: 20.96003532409668
New cuda time without quantization: 11.46049976348877
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.748197555541992
now round: 2
New cuda time without quantization: 7.836493968963623
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.879711151123047
New cuda time without quantization: 6.964171886444092
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.120830535888672
now round: 2
New cuda time without quantization: 6.864972114562988
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.911550521850586
New cuda time without quantization: 17.162111282348633
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.241647720336914
now round: 2
New cuda time without quantization: 13.43106460571289
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.47956085205078
New cuda time without quantization: 7.124011993408203
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.182750701904297
now round: 2
New cuda time without quantization: 6.933931827545166
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.96371078491211
New cuda time without quantization: 11.27202033996582
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.35443878173828
now round: 2
New cuda time without quantization: 6.985773086547852
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.05010986328125
New cuda time without quantization: 6.952332019805908
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.023229598999023
now round: 2
New cuda time without quantization: 6.8328118324279785
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.879390716552734
New cuda time without quantization: 6.848812103271484
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.10563087463379
now round: 3
New cuda time without quantization: 13.681465148925781
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.741161346435547
New cuda time without quantization: 6.93201208114624
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.130109786987305
now round: 3
New cuda time without quantization: 12.49874210357666
torch.Size([524288, 768]) torch.qint8
New cuda time: 22.552839279174805
New cuda time without quantization: 7.015371799468994
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.210269927978516
now round: 3
New cuda time without quantization: 6.762092113494873
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.789627075195312
New cuda time without quantization: 6.9657721519470215
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.044029235839844
now round: 3
New cuda time without quantization: 6.928651809692383
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.9381103515625
New cuda time without quantization: 6.868812084197998
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.991710662841797
now round: 3
New cuda time without quantization: 6.8057708740234375
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.814748764038086
New cuda time without quantization: 12.133301734924316
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.225160598754883
now round: 3
New cuda time without quantization: 6.958733081817627
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.00851058959961
New cuda time without quantization: 6.903532028198242
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.02227020263672
now round: 4
New cuda time without quantization: 7.144012928009033
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.16642951965332
New cuda time without quantization: 6.924972057342529
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.985149383544922
now round: 4
New cuda time without quantization: 9.75473690032959
torch.Size([524288, 768]) torch.qint8
New cuda time: 19.913476943969727
New cuda time without quantization: 7.2998528480529785
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.37539291381836
now round: 4
New cuda time without quantization: 6.898412227630615
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.941789627075195
New cuda time without quantization: 12.482421875
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.570920944213867
now round: 4
New cuda time without quantization: 6.980813026428223
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.015710830688477
New cuda time without quantization: 6.95009183883667
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.011869430541992
now round: 4
New cuda time without quantization: 6.910411834716797
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.955549240112305
New cuda time without quantization: 6.896331787109375
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.169469833374023
now round: 4
New cuda time without quantization: 13.160022735595703
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.181640625
New cuda time without quantization: 7.015851974487305
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.095069885253906
now round: 5
New cuda time without quantization: 12.70930290222168
torch.Size([524288, 768]) torch.qint8
New cuda time: 22.7243595123291
New cuda time without quantization: 7.116652011871338
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.19154930114746
now round: 5
New cuda time without quantization: 6.872171878814697
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.888349533081055
New cuda time without quantization: 6.999691963195801
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.060989379882812
now round: 5
New cuda time without quantization: 6.905451774597168
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.927230834960938
New cuda time without quantization: 6.8326520919799805
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.938108444213867
now round: 5
New cuda time without quantization: 6.855691909790039
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.88530921936035
New cuda time without quantization: 11.30466079711914
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.383556365966797
now round: 5
New cuda time without quantization: 6.962732791900635
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.984189987182617
New cuda time without quantization: 6.965292930603027
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.041149139404297
now round: 5
New cuda time without quantization: 6.9233717918396
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.991710662841797
New cuda time without quantization: 6.976012229919434
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.05714988708496
now round: 6
New cuda time without quantization: 9.558576583862305
now round: 1
New cuda time without quantization: 5355.23681640625
torch.Size([524288, 768]) torch.qint8
New cuda time: 5366.341796875
New cuda time without quantization: 6.692666053771973
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.97782325744629
now round: 1
New cuda time without quantization: 189.3836212158203
torch.Size([524288, 768]) torch.qint8
New cuda time: 199.64877319335938
New cuda time without quantization: 6.952506065368652
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.120384216308594
now round: 1
New cuda time without quantization: 8.703072547912598
torch.Size([524288, 768]) torch.qint8
New cuda time: 18.78263282775879
New cuda time without quantization: 6.772026062011719
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.862144470214844
now round: 1
New cuda time without quantization: 6.5736260414123535
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.587263107299805
New cuda time without quantization: 6.901145935058594
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.07142448425293
now round: 1
New cuda time without quantization: 6.6296257972717285
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.700702667236328
New cuda time without quantization: 12.589967727661133
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.6933650970459
now round: 1
New cuda time without quantization: 6.633144855499268
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.699262619018555
New cuda time without quantization: 11.380043983459473
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.618322372436523
now round: 2
New cuda time without quantization: 6.632345199584961
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.685182571411133
New cuda time without quantization: 6.866265773773193
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.999263763427734
now round: 2
New cuda time without quantization: 6.790904998779297
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.804704666137695
New cuda time without quantization: 17.035425186157227
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.110183715820312
now round: 2
New cuda time without quantization: 13.347891807556152
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.446809768676758
New cuda time without quantization: 6.798585891723633
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.889984130859375
now round: 2
New cuda time without quantization: 6.806105136871338
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.845184326171875
New cuda time without quantization: 11.18708324432373
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.256559371948242
now round: 2
New cuda time without quantization: 6.835546016693115
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.865983963012695
New cuda time without quantization: 6.853147029876709
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.91926383972168
now round: 2
New cuda time without quantization: 6.6848249435424805
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.701984405517578
New cuda time without quantization: 17.618947982788086
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.710025787353516
now round: 3
New cuda time without quantization: 6.821946144104004
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.84102439880371
New cuda time without quantization: 6.901947021484375
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.09062385559082
now round: 3
New cuda time without quantization: 12.384206771850586
torch.Size([524288, 768]) torch.qint8
New cuda time: 22.4224853515625
New cuda time without quantization: 6.882106781005859
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.95030403137207
now round: 3
New cuda time without quantization: 6.871705055236816
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.898624420166016
New cuda time without quantization: 6.901627063751221
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.97382354736328
now round: 3
New cuda time without quantization: 6.785464763641357
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.82294464111328
New cuda time without quantization: 12.551407814025879
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.66152572631836
now round: 3
New cuda time without quantization: 6.917305946350098
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.985183715820312
New cuda time without quantization: 11.990286827087402
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.07848358154297
now round: 3
New cuda time without quantization: 6.84002685546875
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.8787841796875
New cuda time without quantization: 6.914906978607178
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.976543426513672
now round: 4
New cuda time without quantization: 6.599706172943115
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.61734390258789
New cuda time without quantization: 6.881307125091553
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.942943572998047
now round: 4
New cuda time without quantization: 20.585519790649414
torch.Size([524288, 768]) torch.qint8
New cuda time: 30.73963737487793
New cuda time without quantization: 6.709465980529785
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.803903579711914
now round: 4
New cuda time without quantization: 6.873624801635742
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.880224227905273
New cuda time without quantization: 12.287247657775879
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.37144660949707
now round: 4
New cuda time without quantization: 7.003386974334717
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.042463302612305
New cuda time without quantization: 6.976507186889648
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.050464630126953
now round: 4
New cuda time without quantization: 6.738584995269775
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.778303146362305
New cuda time without quantization: 16.768224716186523
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 26.941064834594727
now round: 4
New cuda time without quantization: 6.772826194763184
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.854143142700195
New cuda time without quantization: 6.858427047729492
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.92246437072754
now round: 5
New cuda time without quantization: 11.802125930786133
torch.Size([524288, 768]) torch.qint8
New cuda time: 21.826324462890625
New cuda time without quantization: 6.923867225646973
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.96486473083496
now round: 5
New cuda time without quantization: 6.734745025634766
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.76278305053711
New cuda time without quantization: 6.750905990600586
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.82102394104004
now round: 5
New cuda time without quantization: 6.841465950012207
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.865663528442383
New cuda time without quantization: 10.479080200195312
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 20.607118606567383
now round: 5
New cuda time without quantization: 6.940027236938477
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.96406364440918
New cuda time without quantization: 10.883082389831543
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 20.97991943359375
now round: 5
New cuda time without quantization: 6.846267223358154
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.89958381652832
New cuda time without quantization: 6.957467079162598
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.00454330444336
now round: 5
New cuda time without quantization: 6.794744968414307
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.842144012451172
New cuda time without quantization: 6.861947059631348
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.939104080200195
now round: 6
New cuda time without quantization: 19.729995727539062
torch.Size([524288, 768]) torch.qint8
New cuda time: 20.02419662475586
New cuda time without quantization: 7.07537317276001
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.195709228515625
now round: 6
New cuda time without quantization: 6.811051845550537
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.847389221191406
New cuda time without quantization: 12.331542015075684
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.443239212036133
now round: 6
New cuda time without quantization: 6.917932987213135
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.962270736694336
New cuda time without quantization: 6.935052871704102
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.01618766784668
now round: 6
New cuda time without quantization: 6.891692161560059
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.979869842529297
New cuda time without quantization: 6.877932071685791
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.164831161499023
now round: 6
New cuda time without quantization: 13.396663665771484
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.433481216430664
New cuda time without quantization: 6.950732231140137
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.025630950927734
now round: 6
New cuda time without quantization: 12.982583045959473
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.08884048461914
New cuda time without quantization: 6.955212116241455
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.02819061279297
now round: 7
New cuda time without quantization: 7.619214057922363
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.638111114501953
New cuda time without quantization: 11.92994213104248
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.016679763793945
now round: 7
New cuda time without quantization: 7.08193302154541
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.129789352416992
New cuda time without quantization: 6.813292026519775
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.923389434814453
now round: 7
New cuda time without quantization: 6.8174519538879395
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.835229873657227
New cuda time without quantization: 11.5285005569458
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.61556053161621
now round: 7
New cuda time without quantization: 6.919053077697754
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.955869674682617
New cuda time without quantization: 21.602439880371094
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 31.6939754486084
now round: 7
New cuda time without quantization: 7.108333110809326
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.144990921020508
New cuda time without quantization: 6.915212154388428
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.974430084228516
now round: 7
New cuda time without quantization: 11.865781784057617
torch.Size([524288, 768]) torch.qint8
New cuda time: 21.966760635375977
New cuda time without quantization: 16.881629943847656
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 26.97812843322754
now round: 8
New cuda time without quantization: 7.729773998260498
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.888832092285156
New cuda time without quantization: 7.042253017425537
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.110910415649414
now round: 8
New cuda time without quantization: 6.918091773986816
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.94866943359375
New cuda time without quantization: 6.929612159729004
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.009790420532227
now round: 8
New cuda time without quantization: 16.676349639892578
torch.Size([524288, 768]) torch.qint8
New cuda time: 26.83684730529785
New cuda time without quantization: 6.839211940765381
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.982587814331055
now round: 8
New cuda time without quantization: 6.864652156829834
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.886747360229492
New cuda time without quantization: 6.94241189956665
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.20819091796875
now round: 8
New cuda time without quantization: 13.890905380249023
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.924362182617188
New cuda time without quantization: 16.663070678710938
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 26.754127502441406
now round: 8
New cuda time without quantization: 7.052173137664795
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.073469161987305
New cuda time without quantization: 6.8688130378723145
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.909948348999023
now round: 9
New cuda time without quantization: 6.826891899108887
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.936830520629883
New cuda time without quantization: 17.499391555786133
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.598447799682617
now round: 9
New cuda time without quantization: 11.033140182495117
torch.Size([524288, 768]) torch.qint8
New cuda time: 21.055875778198242
New cuda time without quantization: 11.407541275024414
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.5168399810791
now round: 9
New cuda time without quantization: 6.858413219451904
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.895870208740234
New cuda time without quantization: 7.015693187713623
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.098909378051758
now round: 9
New cuda time without quantization: 6.879211902618408
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.914430618286133
New cuda time without quantization: 6.912652969360352
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.168670654296875
now round: 9
New cuda time without quantization: 11.384180068969727
torch.Size([524288, 768]) torch.qint8
New cuda time: 21.415237426757812
New cuda time without quantization: 6.991373062133789
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.051389694213867
now round: 9
New cuda time without quantization: 12.797943115234375
torch.Size([524288, 768]) torch.qint8
New cuda time: 22.8392391204834
New cuda time without quantization: 15.767389297485352
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 25.96388816833496
now round: 10
New cuda time without quantization: 7.601294040679932
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.62259292602539
New cuda time without quantization: 6.999853134155273
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.07027244567871
now round: 10
New cuda time without quantization: 6.847210884094238
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.872507095336914
New cuda time without quantization: 6.856651782989502
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.978107452392578
now round: 10
New cuda time without quantization: 17.8721923828125
torch.Size([524288, 768]) torch.qint8
New cuda time: 27.937170028686523
New cuda time without quantization: 12.247701644897461
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.33172035217285
now round: 10
New cuda time without quantization: 6.965292930603027
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.9808292388916
New cuda time without quantization: 7.020652770996094
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.1147518157959
now round: 10
New cuda time without quantization: 6.923692226409912
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.966110229492188
New cuda time without quantization: 17.627391815185547
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.719730377197266
now round: 10
New cuda time without quantization: 7.067052841186523
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.11026954650879
New cuda time without quantization: 12.472662925720215
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.550920486450195
now round: 11
New cuda time without quantization: 13.000823974609375
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.31275177001953
torch.Size([524288, 768]) torch.qint8
New cuda time: 27.868240356445312
New cuda time without quantization: 9.337252616882324
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 19.40074348449707
now round: 6
New cuda time without quantization: 6.8348588943481445
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.842992782592773
New cuda time without quantization: 12.475004196166992
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.57369613647461
now round: 6
New cuda time without quantization: 6.830540180206299
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.86939239501953
New cuda time without quantization: 6.90910005569458
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.983951568603516
now round: 6
New cuda time without quantization: 6.718860149383545
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.775951385498047
New cuda time without quantization: 18.056428909301758
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 28.16823959350586
now round: 6
New cuda time without quantization: 13.471001625061035
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.537052154541016
New cuda time without quantization: 6.9175801277160645
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.007312774658203
now round: 6
New cuda time without quantization: 13.004121780395508
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.079614639282227
New cuda time without quantization: 6.7404608726501465
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.810991287231445
now round: 7
New cuda time without quantization: 7.374538898468018
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.384750366210938
New cuda time without quantization: 6.89438009262085
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.12811279296875
now round: 7
New cuda time without quantization: 6.669260025024414
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.701072692871094
New cuda time without quantization: 18.027788162231445
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 28.119279861450195
now round: 7
New cuda time without quantization: 6.8119797706604
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.813392639160156
New cuda time without quantization: 11.463006973266602
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.552257537841797
now round: 7
New cuda time without quantization: 6.877580165863037
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.960912704467773
New cuda time without quantization: 6.825099945068359
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.901391983032227
now round: 7
New cuda time without quantization: 6.847339153289795
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.88043212890625
New cuda time without quantization: 6.925099849700928
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.986671447753906
now round: 7
New cuda time without quantization: 17.77098846435547
torch.Size([524288, 768]) torch.qint8
New cuda time: 27.947439193725586
New cuda time without quantization: 22.026338577270508
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 32.12630844116211
now round: 8
New cuda time without quantization: 7.116620063781738
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.29595184326172
New cuda time without quantization: 6.903180122375488
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.980112075805664
now round: 8
New cuda time without quantization: 6.839019775390625
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.848112106323242
New cuda time without quantization: 6.919179916381836
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.965551376342773
now round: 8
New cuda time without quantization: 6.782859802246094
torch.Size([524288, 768]) torch.qint8
New cuda time: 26.797203063964844
New cuda time without quantization: 15.360596656799316
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 25.569847106933594
now round: 8
New cuda time without quantization: 6.830380916595459
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.84507179260254
New cuda time without quantization: 6.853580951690674
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.047632217407227
now round: 8
New cuda time without quantization: 13.996118545532227
torch.Size([524288, 768]) torch.qint8
New cuda time: 24.034652709960938
New cuda time without quantization: 6.874060153961182
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.105552673339844
now round: 8
New cuda time without quantization: 6.703660011291504
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.754512786865234
New cuda time without quantization: 6.9065399169921875
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.94923210144043
now round: 9
New cuda time without quantization: 16.38923454284668
torch.Size([524288, 768]) torch.qint8
New cuda time: 26.39720344543457
New cuda time without quantization: 21.42218017578125
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 31.67159080505371
now round: 9
New cuda time without quantization: 6.667661190032959
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.735952377319336
New cuda time without quantization: 11.389087677001953
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.478979110717773
now round: 9
New cuda time without quantization: 6.769580841064453
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.820592880249023
New cuda time without quantization: 6.891499996185303
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.96107292175293
now round: 9
New cuda time without quantization: 6.893579006195068
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.92043113708496
New cuda time without quantization: 8.671015739440918
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 18.73546600341797
now round: 9
New cuda time without quantization: 11.360286712646484
torch.Size([524288, 768]) torch.qint8
New cuda time: 21.386499404907227
New cuda time without quantization: 7.102059841156006
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.16219139099121
now round: 9
New cuda time without quantization: 12.762682914733887
torch.Size([524288, 768]) torch.qint8
New cuda time: 22.836896896362305
New cuda time without quantization: 6.865900993347168
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.111312866210938
now round: 10
New cuda time without quantization: 7.139339923858643
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.156431198120117
New cuda time without quantization: 6.793581008911133
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.8677921295166
now round: 10
New cuda time without quantization: 6.794219970703125
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.83019256591797
New cuda time without quantization: 16.882352828979492
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 26.975444793701172
now round: 10
New cuda time without quantization: 6.820140838623047
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.86587142944336
New cuda time without quantization: 12.378684997558594
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.461536407470703
now round: 10
New cuda time without quantization: 6.823340892791748
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.838991165161133
New cuda time without quantization: 6.816780090332031
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.8796329498291
now round: 10
New cuda time without quantization: 6.780779838562012
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.815792083740234
New cuda time without quantization: 6.857901096343994
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.96731185913086
now round: 10
New cuda time without quantization: 6.844779968261719
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.881872177124023
New cuda time without quantization: 12.482364654541016
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.56777572631836
now round: 11
New cuda time without quantization: 13.033082962036133
torch.Size([524288, 768]) torch.qint8
torch.Size([524288, 768]) torch.qint8
New cuda time: 30.171955108642578
New cuda time without quantization: 6.682745933532715
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.776704788208008
now round: 6
New cuda time without quantization: 6.738424777984619
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.755264282226562
New cuda time without quantization: 12.204687118530273
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.28040313720703
now round: 6
New cuda time without quantization: 6.849465847015381
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.897184371948242
New cuda time without quantization: 6.776986122131348
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.849504470825195
now round: 6
New cuda time without quantization: 6.877466201782227
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.93222427368164
New cuda time without quantization: 18.041189193725586
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 28.188907623291016
now round: 6
New cuda time without quantization: 6.843545913696289
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.86934471130371
New cuda time without quantization: 6.947865962982178
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.004064559936523
now round: 6
New cuda time without quantization: 12.727727890014648
torch.Size([524288, 768]) torch.qint8
New cuda time: 22.79096794128418
New cuda time without quantization: 6.912507057189941
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.971904754638672
now round: 7
New cuda time without quantization: 6.592024803161621
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.625503540039062
New cuda time without quantization: 11.8106050491333
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.925683975219727
now round: 7
New cuda time without quantization: 6.9990668296813965
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.025184631347656
New cuda time without quantization: 11.387563705444336
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.51432228088379
now round: 7
New cuda time without quantization: 6.980506896972656
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.01702308654785
New cuda time without quantization: 11.3600435256958
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.44440269470215
now round: 7
New cuda time without quantization: 6.816186904907227
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.840864181518555
New cuda time without quantization: 21.611764907836914
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 31.684282302856445
now round: 7
New cuda time without quantization: 6.836505889892578
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.868703842163086
New cuda time without quantization: 6.9899468421936035
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.13206672668457
now round: 7
New cuda time without quantization: 22.7484073638916
torch.Size([524288, 768]) torch.qint8
New cuda time: 32.801727294921875
New cuda time without quantization: 16.555744171142578
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 26.76410484313965
now round: 8
New cuda time without quantization: 7.188188076019287
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.388225555419922
New cuda time without quantization: 6.771066188812256
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.86854362487793
now round: 8
New cuda time without quantization: 6.831705093383789
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.878143310546875
New cuda time without quantization: 6.747066020965576
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.88742446899414
now round: 8
New cuda time without quantization: 16.571102142333984
torch.Size([524288, 768]) torch.qint8
New cuda time: 26.725862503051758
New cuda time without quantization: 8.945633888244629
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 19.0517520904541
now round: 8
New cuda time without quantization: 6.929145812988281
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.954784393310547
New cuda time without quantization: 6.997786998748779
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.28998565673828
now round: 8
New cuda time without quantization: 13.591411590576172
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.64377212524414
New cuda time without quantization: 16.69174575805664
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 26.764904022216797
now round: 8
New cuda time without quantization: 6.829306125640869
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.849023818969727
New cuda time without quantization: 6.754426956176758
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.9168643951416
now round: 9
New cuda time without quantization: 15.711260795593262
torch.Size([524288, 768]) torch.qint8
New cuda time: 25.771299362182617
New cuda time without quantization: 14.944058418273926
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 25.16921615600586
now round: 9
New cuda time without quantization: 6.919065952301025
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.960384368896484
New cuda time without quantization: 11.231563568115234
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 21.333681106567383
now round: 9
New cuda time without quantization: 6.842106819152832
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.870624542236328
New cuda time without quantization: 6.75938606262207
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.822303771972656
now round: 9
New cuda time without quantization: 6.863224983215332
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.899263381958008
New cuda time without quantization: 19.06615447998047
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 29.23467254638672
now round: 9
New cuda time without quantization: 6.646585941314697
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.77846336364746
New cuda time without quantization: 6.8286662101745605
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.90390396118164
now round: 9
New cuda time without quantization: 12.569487571716309
torch.Size([524288, 768]) torch.qint8
New cuda time: 22.610647201538086
New cuda time without quantization: 15.759100914001465
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 25.92841911315918
now round: 10
New cuda time without quantization: 6.623865127563477
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.656862258911133
New cuda time without quantization: 6.868506908416748
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.931264877319336
now round: 10
New cuda time without quantization: 6.799385070800781
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.813823699951172
New cuda time without quantization: 10.252679824829102
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 20.41927719116211
now round: 10
New cuda time without quantization: 17.989667892456055
torch.Size([524288, 768]) torch.qint8
New cuda time: 28.031307220458984
New cuda time without quantization: 12.081486701965332
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.171123504638672
now round: 10
New cuda time without quantization: 6.793466091156006
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.82262420654297
New cuda time without quantization: 6.932826995849609
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.001663208007812
now round: 10
New cuda time without quantization: 6.722744941711426
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.732223510742188
New cuda time without quantization: 17.66358757019043
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.76298713684082
now round: 10
New cuda time without quantization: 6.8581061363220215
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.886943817138672
New cuda time without quantization: 6.852666854858398
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.987104415893555
now round: 11
New cuda time without quantization: 12.733169555664062
torch.Size([524288, 768]) torch.qint8
New cuda time: torch.Size([524288, 768]) torch.qint8
New cuda time: 28.65001678466797
New cuda time without quantization: 10.266434669494629
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 20.33302879333496
now round: 6
New cuda time without quantization: 7.134422779083252
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.154935836791992
New cuda time without quantization: 6.659701824188232
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.854616165161133
now round: 6
New cuda time without quantization: 7.00402307510376
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.99669647216797
New cuda time without quantization: 7.533945083618164
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.570138931274414
now round: 6
New cuda time without quantization: 7.2061028480529785
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.247575759887695
New cuda time without quantization: 18.823904037475586
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 28.99225616455078
now round: 6
New cuda time without quantization: 13.657965660095215
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.669837951660156
New cuda time without quantization: 7.5221052169799805
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.57541847229004
now round: 6
New cuda time without quantization: 6.830742835998535
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.036376953125
New cuda time without quantization: 7.341464996337891
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.413818359375
now round: 7
New cuda time without quantization: 7.246103763580322
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.272855758666992
New cuda time without quantization: 12.817164421081543
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 22.929676055908203
now round: 7
New cuda time without quantization: 7.00386381149292
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.0461368560791
New cuda time without quantization: 18.563743591308594
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 28.65337562561035
now round: 7
New cuda time without quantization: 7.104824066162109
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.105016708374023
New cuda time without quantization: 6.657782077789307
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.77141571044922
now round: 7
New cuda time without quantization: 7.149783134460449
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.2058162689209
New cuda time without quantization: 22.312715530395508
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 32.405067443847656
now round: 7
New cuda time without quantization: 7.1262640953063965
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.140216827392578
New cuda time without quantization: 7.660665988922119
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.800540924072266
now round: 7
New cuda time without quantization: 17.970619201660156
torch.Size([524288, 768]) torch.qint8
New cuda time: 28.098974227905273
New cuda time without quantization: 18.80518341064453
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 28.879615783691406
now round: 8
New cuda time without quantization: 6.587381839752197
torch.Size([524288, 768]) torch.qint8
New cuda time: 22.970796585083008
New cuda time without quantization: 7.464025020599365
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.512218475341797
now round: 8
New cuda time without quantization: 7.1349029541015625
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.163095474243164
New cuda time without quantization: 7.486905097961426
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.63125991821289
now round: 8
New cuda time without quantization: 16.847896575927734
torch.Size([524288, 768]) torch.qint8
New cuda time: 26.97193145751953
New cuda time without quantization: 16.15813446044922
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 26.342967987060547
now round: 8
New cuda time without quantization: 7.053944110870361
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.061016082763672
New cuda time without quantization: 7.697946071624756
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 25.243444442749023
now round: 8
New cuda time without quantization: 6.7862629890441895
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.80405616760254
New cuda time without quantization: 17.428699493408203
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.522811889648438
now round: 8
New cuda time without quantization: 7.043863773345947
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.095895767211914
New cuda time without quantization: 7.525625228881836
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.670297622680664
now round: 9
New cuda time without quantization: 16.746456146240234
torch.Size([524288, 768]) torch.qint8
New cuda time: 26.762487411499023
New cuda time without quantization: 18.805343627929688
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 28.877376556396484
now round: 9
New cuda time without quantization: 10.876836776733398
torch.Size([524288, 768]) torch.qint8
New cuda time: 20.894468307495117
New cuda time without quantization: 6.642743110656738
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.753175735473633
now round: 9
New cuda time without quantization: 7.048182964324951
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.071895599365234
New cuda time without quantization: 7.602106094360352
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.6682186126709
now round: 9
New cuda time without quantization: 7.19106388092041
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.217975616455078
New cuda time without quantization: 19.56534767150879
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 29.737060546875
now round: 9
New cuda time without quantization: 11.663080215454102
torch.Size([524288, 768]) torch.qint8
New cuda time: 21.689512252807617
New cuda time without quantization: 7.77362585067749
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.81846046447754
now round: 9
New cuda time without quantization: 6.8198628425598145
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.939895629882812
New cuda time without quantization: 16.277015686035156
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 26.468088150024414
now round: 10
New cuda time without quantization: 7.525465965270996
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.530298233032227
New cuda time without quantization: 7.722105979919434
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.79397964477539
now round: 10
New cuda time without quantization: 7.076663017272949
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.073495864868164
New cuda time without quantization: 17.54677963256836
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.61785125732422
now round: 10
New cuda time without quantization: 18.06998062133789
torch.Size([524288, 768]) torch.qint8
New cuda time: 28.146974563598633
New cuda time without quantization: 6.655383110046387
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.767894744873047
now round: 10
New cuda time without quantization: 7.08658504486084
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.125499725341797
New cuda time without quantization: 7.667386054992676
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.730140686035156
now round: 10
New cuda time without quantization: 7.145143032073975
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.166776657104492
New cuda time without quantization: 18.59734344482422
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 28.684736251831055
now round: 10
New cuda time without quantization: 7.0888237953186035
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.099895477294922
New cuda time without quantization: 13.327884674072266
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 23.391117095947266
now round: 11
New cuda time without quantization: 14.251568794250488
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.02596092224121
New cuda time without quantization: 20.48163604736328
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 30.56421661376953
now round: 11
New cuda time without quantization: 6.961933135986328
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.007869720458984
New cuda time without quantization: 6.939692974090576
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.014751434326172
now round: 11
New cuda time without quantization: 6.856331825256348
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.89250946044922
New cuda time without quantization: 6.988492965698242
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.07539176940918
now round: 11
New cuda time without quantization: 13.405624389648438
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.446441650390625
New cuda time without quantization: 18.130111694335938
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 28.205169677734375
now round: 11
New cuda time without quantization: 12.360981941223145
torch.Size([524288, 768]) torch.qint8
New cuda time: 22.384679794311523
New cuda time without quantization: 6.922892093658447
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.08675193786621
now round: 11
New cuda time without quantization: 6.915852069854736
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.962909698486328
New cuda time without quantization: 16.53091049194336
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 26.649168014526367
2024-07-12 03:46:28 | WARNING | fairseq_cli.eval_lm | Aggregating scores across the distributed world
24.27783966064453
New cuda time without quantization: 20.360069274902344
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 30.422502517700195
now round: 11
New cuda time without quantization: 7.131703853607178
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.134775161743164
New cuda time without quantization: 7.419226169586182
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.47557830810547
now round: 11
New cuda time without quantization: 7.147223949432373
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.16165542602539
New cuda time without quantization: 7.583866119384766
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.638139724731445
now round: 11
New cuda time without quantization: 13.724685668945312
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.74888038635254
New cuda time without quantization: 18.732223510742188
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 28.798816680908203
now round: 11
New cuda time without quantization: 6.612183094024658
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.633014678955078
New cuda time without quantization: 7.65138578414917
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.79046058654785
now round: 11
New cuda time without quantization: 7.1710638999938965
torch.Size([524288, 768]) torch.qint8
New cuda time: 17.184215545654297
New cuda time without quantization: 17.372379302978516
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.446491241455078
2024-07-12 03:46:28 | WARNING | fairseq_cli.eval_lm | Aggregating scores across the distributed world
22.773527145385742
New cuda time without quantization: 19.53943634033203
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 29.755634307861328
now round: 11
New cuda time without quantization: 6.796986103057861
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.84342384338379
New cuda time without quantization: 6.879065990447998
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.931903839111328
now round: 11
New cuda time without quantization: 6.840025901794434
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.839584350585938
New cuda time without quantization: 6.897626876831055
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.976064682006836
now round: 11
New cuda time without quantization: 6.832187175750732
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.88934326171875
New cuda time without quantization: 17.85542869567871
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 27.95050811767578
now round: 11
New cuda time without quantization: 12.117326736450195
torch.Size([524288, 768]) torch.qint8
New cuda time: 22.146644592285156
New cuda time without quantization: 7.042427062988281
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.189985275268555
now round: 11
New cuda time without quantization: 6.733304977416992
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.768064498901367
New cuda time without quantization: 16.448863983154297
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 26.526662826538086
2024-07-12 03:46:28 | WARNING | fairseq_cli.eval_lm | Aggregating scores across the distributed world
New cuda time: 23.052576065063477
New cuda time without quantization: 6.904780864715576
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.9826717376709
now round: 11
New cuda time without quantization: 6.800300121307373
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.833391189575195
New cuda time without quantization: 6.904460906982422
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.99627113342285
now round: 11
New cuda time without quantization: 6.837900161743164
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.847471237182617
New cuda time without quantization: 6.844301223754883
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.906192779541016
now round: 11
New cuda time without quantization: 13.260441780090332
torch.Size([524288, 768]) torch.qint8
New cuda time: 23.33017349243164
New cuda time without quantization: 6.914540767669678
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.006511688232422
now round: 11
New cuda time without quantization: 12.334203720092773
torch.Size([524288, 768]) torch.qint8
New cuda time: 22.378976821899414
New cuda time without quantization: 6.777260780334473
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 16.921232223510742
now round: 11
New cuda time without quantization: 6.827660083770752
torch.Size([524288, 768]) torch.qint8
New cuda time: 16.87099266052246
New cuda time without quantization: 6.861741065979004
torch.Size([4, 128, 1024, 768]) torch.qint8
New cuda time: 17.070192337036133
2024-07-12 03:46:28 | WARNING | fairseq_cli.eval_lm | Aggregating scores across the distributed world
2024-07-12 03:46:28 | WARNING | fairseq_cli.eval_lm | Aggregating timer stats across the distributed world
2024-07-12 03:46:28 | WARNING | fairseq_cli.eval_lm | Aggregating timer stats across the distributed world
2024-07-12 03:46:28 | WARNING | fairseq_cli.eval_lm | Aggregating timer stats across the distributed world
2024-07-12 03:46:28 | WARNING | fairseq_cli.eval_lm | Aggregating timer stats across the distributed world
2024-07-12 03:46:28 | INFO | fairseq_cli.eval_lm | Evaluated 899,369 tokens in 37.2s (24176.21 tokens/s)
2024-07-12 03:46:28 | INFO | fairseq_cli.eval_lm | valid Loss (base 2): 2.9631, Perplexity: 7.80
/usr/lib64/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
